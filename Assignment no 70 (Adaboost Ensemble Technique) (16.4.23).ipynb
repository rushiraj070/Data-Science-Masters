{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ead8528-fdbd-427e-935c-56e5a692d74f",
   "metadata": {},
   "source": [
    "# Assignment no 70 (Adaboost Ensemble Technique) (16.4.23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf588ab7-cd85-469e-805a-2ca7f7c3fb81",
   "metadata": {},
   "source": [
    "### Q1. What is boosting in machine learning?\n",
    "\n",
    "**Ans-** \n",
    "1. Boosting is an ensemble learning method that **combines a set of weak learners into a strong learner to minimize training errors.** \n",
    "2. In boosting, a random sample of data is selected, fitted with a model and then trained sequentially—that is, **each model tries to compensate for the weaknesses of its predecessor.** \n",
    "3. With each iteration, the weak rules from each individual classifier are combined to form one, strong prediction rule. \n",
    "4. Boosting is primarily used in machine learning **for reducing bias and variance.**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1576e481-f009-4a00-a578-5683bc1c37af",
   "metadata": {},
   "source": [
    "Source: Conversation with Bing, 30/7/2023\n",
    "(1) What is Boosting? | IBM. https://www.ibm.com/topics/boosting.\n",
    "(2) What are Boosting Algorithms and how they work. https://towardsmachinelearning.org/boosting-algorithms/.\n",
    "(3) . https://bing.com/search?q=boosting+in+machine+learning.\n",
    "(4) Boosting in Machine Learning | Boosting and AdaBoost. https://www.geeksforgeeks.org/boosting-in-machine-learning-boosting-and-adaboost/.\n",
    "(5) Boosting (machine learning) - Wikipedia. https://en.wikipedia.org/wiki/Boosting_%28machine_learning%29.\n",
    "(6) What Is Boosting in Machine Learning: A Comprehensive Guide - Simplilearn. https://www.simplilearn.com/tutorials/machine-learning-tutorial/what-is-boosting.\n",
    "(7) undefined. https://www.analyticsvidhya.com/blog/2021/09/adaboost-algorithm-a-complete-guide-for-beginners/.\n",
    "(8) undefined. https://aws.amazon.com/what-is/boosting/.\n",
    "(9) undefined. https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be690f94-7003-4781-8ec6-a8dd43ba70ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98cc784d-00da-4567-981d-6ba0bc15b319",
   "metadata": {},
   "source": [
    "### Q2. What are the advantages and limitations of using boosting techniques?\n",
    "\n",
    "**Ans-** Boosting is an ensemble learning method that combines a set of weak learners into a strong learner to minimize training errors. Here are some advantages and disadvantages of using boosting techniques:\n",
    "\n",
    "### Advantages:\n",
    "1. **Improved Accuracy:** Boosting can improve the accuracy of the model by combining several weak models’ accuracies and averaging them for regression or voting over them for classification to increase the accuracy of the final model.\n",
    "2. **Robustness to Overfitting:** Boosting can reduce the risk of overfitting by reweighting the inputs that are classified wrongly.\n",
    "3. **Better handling of imbalanced data:** Boosting can handle the imbalance data by focusing more on the data points that are misclassified.\n",
    "4. **Better Interpretability:** Boosting can increase the interpretability of the model by breaking the model decision process into multiple processes.\n",
    "\n",
    "### Limitations:\n",
    "1. **Overfitting or Variance Issues:** Boosting technique often ignores overfitting or variance issues in the data set.\n",
    "2. **Increased Complexity:** It increases the complexity of the classification."
   ]
  },
  {
   "cell_type": "raw",
   "id": "11cd8517-badd-416e-b1e1-592fcfe14ccb",
   "metadata": {},
   "source": [
    "Source: Conversation with Bing, 30/7/2023\n",
    "(1) Boosting in Machine Learning | Boosting and AdaBoost. https://www.geeksforgeeks.org/boosting-in-machine-learning-boosting-and-adaboost/.\n",
    "(2) Bagging and Boosting method - Medium. https://medium.com/@ruhi3929/bagging-and-boosting-method-c036236376eb.\n",
    "(3) Boosting - Overview, Forms, Pros and Cons, Option Trees. https://corporatefinanceinstitute.com/resources/data-science/boosting/.\n",
    "(4) Bagging and Boosting | Most Used Techniques of Ensemble Learning - EDUCBA. https://www.educba.com/bagging-and-boosting/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cc7fe9-31c0-4a85-b3dc-e3823b378965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "300fb547-8076-4ed4-bb77-2096f8f2ee36",
   "metadata": {},
   "source": [
    "### Q3. Explain how boosting works.\n",
    "\n",
    "**Ans-** Boosting is an ensemble learning method that combines a set of weak learners into a strong learner to minimize training errors. \n",
    "\n",
    "**Here's how boosting works:**\n",
    "\n",
    "1. A random sample of data is selected and fitted with a model.\n",
    "2. The model is trained on the data and the errors are calculated.\n",
    "3. The data points that were misclassified are given more weight in the next iteration.\n",
    "4. Another model is trained on the reweighted data and the errors are calculated again.\n",
    "5. This process is repeated until the desired accuracy is achieved or the maximum number of iterations is reached.\n",
    "6. The final model is a weighted combination of all the models trained in each iteration.\n",
    "\n",
    "The idea behind boosting is to focus on the data points that are difficult to classify correctly, and to give them more importance in the training process. By doing this, the model can learn to classify these data points correctly, thus improving its overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81c6fc0-4544-4a08-b2c0-a710a18f6d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6560719-9f04-42ed-822c-8bb33a52477f",
   "metadata": {},
   "source": [
    "### Q4. What are the different types of boosting algorithms?\n",
    "\n",
    "**Ans-** There are several types of boosting algorithms that are used to improve the accuracy of machine learning models. Some examples of popular boosting algorithms include:\n",
    "\n",
    "1. **Gradient Boosting**: This algorithm helps improve the accuracy of machine learning models by using a loss function to measure the difference between expected and actual outputs.\n",
    "\n",
    "2. **AdaBoost (Adaptive Boosting)**: This algorithm prioritizes (assigns weight) the mistakes while examining which datapoints provided the most inaccurate predictions.\n",
    "\n",
    "3. **XGBoost (Extreme Gradient Boosting)**: This is a modified version of the Gradient Boosting algorithm that provides a gradient boosting framework for C++, Java, Python, R, and Julia.\n",
    "\n",
    "4. **CatBoost (Categorical Boosting)**: This is a gradient boosting algorithm that can handle categorical features automatically and is designed to be fast and scalable.\n",
    "\n",
    "5. **LightGBM (Light Gradient Boosting Machine)**: This is another gradient boosting algorithm that is designed to be faster and more efficient than traditional gradient boosting algorithms.\n",
    "\n",
    "Each of these algorithms has its own mechanism and uses that differ from the others."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f6eb9098-3b98-4312-b9b3-8897a6673fa4",
   "metadata": {},
   "source": [
    "Source: Conversation with Bing, 30/7/2023\n",
    "(1) Types Of Boosting Algorithms (Which technique is the best for you). https://www.analyticsfordecisions.com/types-of-boosting-algorithms/.\n",
    "(2) undefined. https://www.analyticsvidhya.com/blog/2022/12/ultimate-guide-to-boosting-algorithms/.\n",
    "(3) . https://bing.com/search?q=types+of+boosting+algorithms.\n",
    "(4) What is Boosting? | IBM. https://www.ibm.com/topics/boosting.\n",
    "(5) Types of Boosting Algorithm With Their Working - EDUCBA. https://www.educba.com/boosting-algorithm/.\n",
    "(6) undefined. https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/.\n",
    "(7) undefined. https://towardsdatascience.com/boosting-algorithms-explained-d38f56ef3f30.\n",
    "(8) undefined. https://towardsmachinelearning.org/boosting-algorithms/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f903cc9c-ef1d-4de9-8610-e4b9e4efb0b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7a8c798-8775-4525-a6e2-f49b26b307c8",
   "metadata": {},
   "source": [
    "### Q5. What are some common parameters in boosting algorithms?\n",
    "\n",
    "**Ans-** Some common parameters in boosting algorithms include:\n",
    "\n",
    "1. **Learning rate**: This parameter controls the contribution of each weak learner in the final model. A smaller learning rate means that each weak learner has less influence, and the model is more robust to overfitting.\n",
    "\n",
    "2. **Max depth**: This parameter controls the maximum depth of the individual decision trees used as weak learners. A smaller max depth can help prevent overfitting.\n",
    "\n",
    "3. **Number of estimators**: This parameter controls the number of weak learners used in the final model. A larger number of estimators can improve the accuracy of the model, but it can also increase the risk of overfitting.\n",
    "\n",
    "These are some of the most common parameters that are used to tune boosting algorithms."
   ]
  },
  {
   "cell_type": "raw",
   "id": "472c93f3-678a-4298-831a-ab6890bb8d99",
   "metadata": {},
   "source": [
    "Source: Conversation with Bing, 30/7/2023\n",
    "(1) Gradient Boosting Hyperparameters Tuning : Classifier Example. https://www.datasciencelearner.com/gradient-boosting-hyperparameters-tuning/.\n",
    "(2) . https://bing.com/search?q=common+parameters+in+boosting+algorithms.\n",
    "(3) Boosting Algorithm | Boosting Algorithms in Machine Learning. https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/.\n",
    "(4) Boosting Algorithms: A Review of Methods, Theory, and Applications. https://link.springer.com/chapter/10.1007/978-1-4419-9326-7_2.\n",
    "(5) Best Boosting Algorithm In Machine Learning In 2021 - Analytics Vidhya. https://www.analyticsvidhya.com/blog/2021/04/best-boosting-algorithm-in-machine-learning-in-2021/.\n",
    "(6) undefined. https://www.analyticsvidhya.com/blog/2020/02/4-boosting-algorithms-machine-learning/.\n",
    "(7) undefined. https://towardsdatascience.com/boosting-algorithms-explained-d38f56ef3f30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c54a42-6d17-4cb4-b270-85694a4c4ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31c1d4d1-d691-4151-9a75-36356927dbab",
   "metadata": {},
   "source": [
    "### Q6. How do boosting algorithms combine weak learners to create a strong learner?\n",
    "\n",
    "**Ans-** Boosting algorithms combine weak learners to create a strong learner by training them sequentially, with each model trying to compensate for the weaknesses of its predecessor. The idea is to focus on the data points that are difficult to classify correctly, and to give them more importance in the training process. By doing this, the model can learn to classify these data points correctly, thus improving its overall accuracy.\n",
    "\n",
    "In each iteration, a new weak learner is trained on the data and the errors are calculated. The data points that were misclassified are given more weight in the next iteration, so that the next weak learner focuses more on these difficult-to-classify data points. This process is repeated until the desired accuracy is achieved or the maximum number of iterations is reached.\n",
    "\n",
    "The final model is a weighted combination of all the models trained in each iteration. The weights are determined by how well each weak learner performed on the training data. The better a weak learner performed, the higher its weight in the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662361b1-cab3-4f31-84df-d14697cb1455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52f93e7c-a26b-49da-ba7d-230754820f1f",
   "metadata": {},
   "source": [
    "### Q7. Explain the concept of AdaBoost algorithm and its working.\n",
    "\n",
    "**Ans-** \n",
    "AdaBoost, short for Adaptive Boosting, is a machine learning algorithm that can be used as an ensemble method. \n",
    "\n",
    "It is called Adaptive Boosting because the weights are re-assigned to each instance, with higher weights assigned to incorrectly classified instances. \n",
    "\n",
    "The algorithm works by combining multiple weak learners into a single strong learner. \n",
    "\n",
    "The most common estimator used with AdaBoost is decision trees with one level, which means decision trees with only one split. \n",
    "\n",
    "These trees are also called Decision Stumps.\n",
    "\n",
    "Here's how the AdaBoost algorithm works:\n",
    "1. Initially, AdaBoost selects a training subset randomly.\n",
    "2. It iteratively trains the AdaBoost machine learning model by selecting the training set based on the accurate prediction of the last training.\n",
    "3. It assigns higher weights to points that are wrongly classified.\n",
    "4. Now all the points with higher weights are given more importance in the next model.\n",
    "5. It will keep training models until and unless a lower error is received¹.\n",
    "\n",
    "The basic idea behind AdaBoost is to set the weights of classifiers and train the data sample in each iteration such that it ensures the accurate predictions of unusual observations."
   ]
  },
  {
   "cell_type": "raw",
   "id": "cda4a93d-6632-4565-9f6d-017018d907aa",
   "metadata": {},
   "source": [
    "Source: Conversation with Bing, 30/7/2023\n",
    "(1) AdaBoost Algorithm: Boosting Algorithm in Machine Learning. https://www.mygreatlearning.com/blog/adaboost-algorithm/.\n",
    "(2) AdaBoost Algorithm: Understand, Implement and Master AdaBoost. https://www.analyticsvidhya.com/blog/2021/09/adaboost-algorithm-a-complete-guide-for-beginners/.\n",
    "(3) AdaBoost - An Introduction to AdaBoost - Machine Learning Plus. https://www.machinelearningplus.com/machine-learning/introduction-to-adaboost/.\n",
    "(4) AdaBoost - Wikipedia. https://en.wikipedia.org/wiki/AdaBoost.\n",
    "(5) Understanding the AdaBoost Algorithm | Built In. https://builtin.com/machine-learning/adaboost.\n",
    "(6) . https://bing.com/search?q=AdaBoost+algorithm+working.\n",
    "(7) AdaBoost Classifier Algorithms using Python Sklearn Tutorial. https://www.datacamp.com/tutorial/adaboost-classifier-python.\n",
    "(8) undefined. https://www.geeksforgeeks.org/boosting-in-machine-learning-boosting-and-adaboost/.\n",
    "(9) undefined. https://pythongeeks.org/adaboost-algorithm-in-machine-learning/.\n",
    "(10) undefined. https://www.geeksforgeeks.org/implementing-the-adaboost-algorithm-from-scratch/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375b327b-407a-433b-9959-edf0ac47ed69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25a63a42-5f37-4c67-8c17-0f3bb51d312f",
   "metadata": {},
   "source": [
    "### Q8. What is the loss function used in AdaBoost algorithm?\n",
    "\n",
    "**Ans-** The loss function used in the AdaBoost algorithm is the exponential loss function. AdaBoost can be shown to be equivalent to forward stagewise additive modeling using an exponential loss function. \n",
    "\n",
    "The exponential loss function is defined as the exponent of minus y times f(x), where y is the true label and f(x) is the predicted value."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e18e133-bb26-4724-9c20-b56fc9e3a65d",
   "metadata": {},
   "source": [
    "Source: Conversation with Bing, 30/7/2023\n",
    "(1) Which Loss Function does Adaboost actually optimize?. https://medium.com/codex/which-loss-function-does-adaboost-actually-optimise-6c645527127f.\n",
    "(2) sklearn.ensemble - scikit-learn 1.3.0 documentation. https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html.\n",
    "(3) A Dynamic AdaBoost Algorithm With Adaptive Changes of Loss Function .... https://ieeexplore.ieee.org/abstract/document/6392455/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39415a30-08a0-4845-acbc-bffab004f873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed8b9570-dd16-47f4-87c9-f4c86477b611",
   "metadata": {},
   "source": [
    "### Q9. How does the AdaBoost algorithm update the weights of misclassified samples?\n",
    "\n",
    "**Ans-** The AdaBoost algorithm updates the weights of misclassified samples by increasing their importance in the next iteration of the training process. Here's how it works:\n",
    "\n",
    "1. Initially, all samples are assigned equal weights.\n",
    "2. A weak learner is trained on the data and the errors are calculated.\n",
    "3. The weights of the misclassified samples are increased, while the weights of the correctly classified samples are decreased.\n",
    "4. The weak learner is retrained on the reweighted data.\n",
    "5. This process is repeated until the desired accuracy is achieved or the maximum number of iterations is reached.\n",
    "\n",
    "The idea behind this approach is to give more importance to the samples that are difficult to classify correctly, so that the weak learner can focus more on these samples in the next iteration. By doing this, the weak learner can learn to classify these samples correctly, thus improving its overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf44db8-928d-448c-9f01-1df2da95445a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b2cded7-31f3-4a6e-819a-d753ec35805b",
   "metadata": {},
   "source": [
    "### Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?\n",
    "\n",
    "**Ans-** \n",
    "- Increasing the number of estimators in the AdaBoost algorithm can improve the accuracy of the model, but it can also increase the risk of overfitting. \n",
    "- The number of estimators refers to the number of weak learners used in the final model. \n",
    "- A larger number of estimators can improve the accuracy of the model by combining the predictions of multiple weak learners. \n",
    "- However, if the number of estimators is too large, the model may start to memorize the training data, leading to overfitting. \n",
    "- Overfitting occurs when the model performs well on the training data but poorly on new, unseen data. \n",
    "- To avoid overfitting, it is important to carefully choose the number of estimators and to use techniques such as cross-validation to evaluate the performance of the model on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5761be05-5a50-4cdd-baad-3a8ad7d58b88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
