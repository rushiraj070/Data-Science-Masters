{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a34d049-8cf8-419e-ab4f-df34d50a7779",
   "metadata": {},
   "source": [
    "## Assignment no 40 Machine Learning  (Intro 2) (16.3.23).ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa0566a-ee65-42c8-804b-053be9e05559",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "can they be mitigated?\n",
    "\n",
    "Ans - Model fitting is a technique used in Machine Learning to check the prediction accuracy of trained model for the \n",
    "    train and test dataset.\n",
    "    \n",
    "    1. Overfitting - When the accuracy is LOW of test dataset with respect to train dataset then the model is said to be \n",
    "                    overfitted and consequence of this type of model is the train dataset has HIGH ACCURACY but LOW BIAS in\n",
    "                    nature in the other hand the test dataset has LOW ACCURACY and HIGH VARIANCE for prediction.\n",
    "        Overfitting happens when:\n",
    "            a. The data used for training is not cleaned and contains garbage values. The model captures the noise in the \n",
    "                training data and fails to generalize the model's learning.\n",
    "            b. The model has a high variance.\n",
    "            c. The training data size is not enough, and the model trains on the limited training data for several epochs.\n",
    "            d. The architecture of the model has several neural layers stacked together. Deep neural networks are complex \n",
    "                and require a significant amount of time to train, and often lead to overfitting the training set.\n",
    "\n",
    "    1. Underfitting - When the accuracy is LOW of test and train dataset then the model is said to be underfitted and \n",
    "                    consequence of this type of model is the train dataset is HIGHLY BIASed in\n",
    "                    nature in the other hand the test dataset has HIGH VARIANCE for prediction.                \n",
    "        Underfitting happens when:\n",
    "            a. Unclean training data containing noise or outliers can be a reason for the model not being able to derive \n",
    "                patterns from the dataset.\n",
    "            b. The model has a high bias due to the inability to capture the relationship between the input examples and \n",
    "                the target values. \n",
    "            c. The model is assumed to be too simple. For example, training a linear model in complex scenarios.\n",
    "\n",
    "     Handling Overfitting:\n",
    "        Train with more data. \n",
    "        Data augmentation. \n",
    "        Addition of noise to the input data. \n",
    "        Feature selection. \n",
    "        Cross-validation.\n",
    "        Simplify data.\n",
    "        Regularization.\n",
    "        Ensembling.\n",
    "        \n",
    "    Handling Underfitting:\n",
    "        Get more training data.\n",
    "        Increase the size or number of parameters in the model.\n",
    "        Increase the complexity of the model.\n",
    "        Increasing the training time, until cost function is minimised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbcbf2f-78d9-4bde-9cf8-68646e5aabd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2: How can we reduce overfitting? Explain in brief.\n",
    "\n",
    "Handling Overfitting:\n",
    "\n",
    "    There are a number of techniques that machine learning researchers can use to mitigate overfitting. These include :\n",
    "\n",
    "    1. Cross-validation\n",
    "        This is done by splitting your dataset into ‘test’ data and ‘train’ data. Build the model using the ‘train’ set. \n",
    "    The ‘test’ set is used for in-time validation. This way you know what the expected output is and you will easily be able\n",
    "    to judge the accuracy of your model.\n",
    "\n",
    "    2. Regularization\n",
    "        This is a form of regression, that regularizes or shrinks the coefficient estimates towards zero. This technique \n",
    "    discourages learning a more complex model.\n",
    "\n",
    "    3. Early stopping\n",
    "        When training a learner with an iterative method, you stop the training process before the final iteration. This \n",
    "    prevents the model from memorizing the dataset.\n",
    "\n",
    "    4. Pruning\n",
    "        This technique applies to decision trees.\n",
    "        Pre-pruning: Stop ‘growing’ the tree earlier before it perfectly classifies the training set.\n",
    "        Post-pruning: Allows the tree to ‘grow’, perfectly classify the training set and then post prune the tree.\n",
    "\n",
    "    5. Dropout\n",
    "        This is a technique where randomly selected neurons are ignored during training.\n",
    "\n",
    "    6. Regularize the weights.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f800c0-8748-4b7f-8f69-78e866eee75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
    "\n",
    "Ans - \n",
    "    Underfitting: A statistical model or a machine learning algorithm is said to have underfitting when it cannot capture the \n",
    "                underlying trend of the data, i.e., it only performs well on training data but performs poorly on testing \n",
    "                data.\n",
    "            \n",
    "        Reasons for Underfitting:\n",
    "            High bias and low variance \n",
    "            The size of the training dataset used is not enough.\n",
    "            The model is too simple.\n",
    "            Training data is not cleaned and also contains noise in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6781c028-0542-424d-b5a5-3a73e91a4931",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "variance, and how do they affect model performance?\n",
    "\n",
    "Ans - Bias is the difference between the average prediction of our model and the correct value which we are trying to \n",
    "    predict. Model with high bias pays very little attention to the training data and oversimplifies the model. It always \n",
    "    leads to high error on training and test data.\n",
    "        Variance is the variability of model prediction for a given data point or a value which tells us spread of our data.\n",
    "    Model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t \n",
    "    seen before. As a result, such models perform very well on training data but has high error rates on test data.\n",
    "\n",
    "    Mathematically\n",
    "        Let the variable we are trying to predict as Y and other covariates as X. We assume there is a relationship between \n",
    "    the two such that\n",
    "        Y=f(X) + e\n",
    "\n",
    "    Where e is the error term and it’s normally distributed with a mean of 0.\n",
    "\n",
    "    We will make a model f^(X) of f(X) using linear regression or any other modeling technique.\n",
    "\n",
    "Bias Variance Tradeoff\n",
    "      If our model is too simple and has very few parameters then it may have high bias and low variance. On the other hand \n",
    "    if our model has large number of parameters then it’s going to have high variance and low bias. So we need to find the \n",
    "    right/good balance without overfitting and underfitting the data.\n",
    "        This tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm can’t be more \n",
    "    complex and less complex at the same time.\n",
    "\n",
    "Total Error\n",
    "    To build a good model, we need to find a good balance between bias and variance such that it minimizes the total error.\n",
    "\n",
    "    An optimal balance of bias and variance would never overfit or underfit the model.\n",
    "\n",
    "Therefore understanding bias and variance is critical for understanding the behavior of prediction models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8956ea90-55dd-4e55-b2dd-a619abce9718",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "How can you determine whether your model is overfitting or underfitting?\n",
    "\n",
    "Ans -\n",
    "    1. Using training and validation curves: Plotting the training and validation curves of a model can help detect \n",
    "    overfitting and underfitting. If the training error is much lower than the validation error, it indicates that the \n",
    "    model is overfitting. If both the training and validation errors are high, it indicates that the model is underfitting.\n",
    "\n",
    "    2. Using learning curves: Learning curves show how the model's performance improves as the size of the training data \n",
    "    increases. If the learning curve plateaus, it indicates that the model is unable to learn from additional training data,\n",
    "    and the model may be underfitting. On the other hand, if the gap between the training and validation curves is large, it \n",
    "    indicates that the model may be overfitting.\n",
    "\n",
    "    3. Using cross-validation: Cross-validation is a technique for evaluating the performance of a model on multiple subsets \n",
    "    of the training data. If the model performs well on all the subsets, it indicates that the model is not overfitting. If \n",
    "    the performance is poor on all subsets, it indicates that the model is underfitting.\n",
    "\n",
    "    4. Using regularization: Regularization techniques such as L1 and L2 regularization can help reduce overfitting by adding \n",
    "    a penalty term to the loss function. If the regularization parameter is too high, it can lead to underfitting.\n",
    "\n",
    "    5. To determine whether a model is overfitting or underfitting, we can use the above methods to analyze the model's \n",
    "    performance. If the training error is low, but the validation error is high, it indicates that the model is overfitting. \n",
    "    If both the training and validation errors are high, it indicates that the model is underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bc0a12-6534-4787-920d-0833ec481dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "and high variance models, and how do they differ in terms of their performance?\n",
    "\n",
    "Ans - \n",
    "      Bias and variance are two sources of error in machine learning models. Bias refers to the difference between the expected\n",
    "    output and the true output of the model, while variance refers to the variability of the model's output for different \n",
    "    inputs.\n",
    "\n",
    "      High bias models are typically too simple and unable to capture the underlying patterns in the data. They tend to \n",
    "    underfit the data, leading to high training and test errors. High bias models have low complexity and often have fewer \n",
    "    parameters than the data requires.\n",
    "    Examples of high bias models include linear regression models, which assume that the relationship between the inputs and\n",
    "    the output is linear, even when it is not.\n",
    "\n",
    "      High variance models are typically too complex and able to fit the training data too closely, including noise in the \n",
    "    data. They tend to overfit the data, leading to low training error but high test error. High variance models have high \n",
    "    complexity and often have more parameters than necessary.\n",
    "    Examples of high variance models include decision trees with deep and complex branches, which can fit the training data \n",
    "    too closely.\n",
    "\n",
    "      The main difference between high bias and high variance models is their performance. High bias models perform poorly on \n",
    "    both training and test data, while high variance models perform well on training data but poorly on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48fac88-61e4-41f3-b21e-3674ce0b548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
    "some common regularization techniques and how they work.\n",
    "\n",
    "Ams - \n",
    "    Regularization is a technique used in machine learning to prevent overfitting, which occurs when a model is too complex and \n",
    "performs well on the training data but poorly on new, unseen data. Regularization works by adding a penalty term to the loss\n",
    "function that encourages the model to have smaller weights, making it less complex and more likely to generalize well to new\n",
    "data.\n",
    "\n",
    "Some common regularization techniques used in machine learning are:\n",
    "\n",
    "1. L1 regularization (Lasso): L1 regularization adds a penalty term proportional to the absolute value of the weights to the \n",
    "loss function. This encourages the model to have sparse weights, i.e., many weights are zero. L1 regularization can be used \n",
    "for feature selection, where only the most important features are used in the model.\n",
    "\n",
    "2. L2 regularization (Ridge): L2 regularization adds a penalty term proportional to the square of the weights to the loss \n",
    "function. This encourages the model to have smaller weights, but it does not lead to sparse weights like L1 regularization. \n",
    "L2 regularization is commonly used in linear regression models.\n",
    "\n",
    "3. Elastic Net: Elastic Net combines L1 and L2 regularization by adding a penalty term proportional to the sum of the absolute \n",
    "and square of the weights to the loss function. This provides a balance between L1 and L2 regularization and can be useful\n",
    "when there are many correlated features in the data.\n",
    "\n",
    "4. Dropout: Dropout is a technique used in deep neural networks that randomly drops out some of the neurons during training. \n",
    "This encourages the model to learn more robust features and reduces overfitting.\n",
    "\n",
    "5. Early stopping: Early stopping is a technique that stops training the model when the performance on the validation set\n",
    "starts to degrade. This helps to prevent the model from overfitting by stopping the training before the model starts to\n",
    "memorize the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca12d7f-b87c-47b9-9c8c-3b946be566be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
