{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71f4544a-ab3c-449b-8b35-afe3bdae9e0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <div class=\"alert alert-success\"> Assignment no 51 (Ridge Regression) (28.3.23) </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d429016-6255-4b12-80ee-088283b159ae",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-warning\"> Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression? </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7b5954-ae13-4c45-9948-2008c2116d67",
   "metadata": {},
   "source": [
    "### Ans - \n",
    "Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L2 regularization. When the issue of multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values being far away from the actual values. \n",
    "\n",
    "The cost function for ridge regression:\n",
    "             n      _         n\n",
    "J(ϴ) = (1/n) Σ (yi– y)**2 + λ Σ (slope)**2\n",
    "            i=1              i=1\n",
    "            \n",
    "Lambda is the penalty term. λ given here is denoted by an alpha parameter in the ridge function. So, by changing the values of alpha, we are controlling the penalty term. The higher the values of alpha, the bigger is the penalty and therefore the magnitude of coefficients is reduced.\n",
    "\n",
    "- It shrinks the parameters. Therefore, it is used to prevent multicollinearity\n",
    "- It reduces the model complexity by coefficient shrinkage\n",
    "\n",
    "Linear Regression establishes a relationship between dependent variable (Y) and one or more independent variables (X) using a best fit straight line (also known as regression line). Ridge Regression is a technique used when the data suffers from multicollinearity ( independent variables are highly correlated)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce11b865-dffd-48ab-83fb-f24baa30cd84",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-info\"> Q2. What are the assumptions of Ridge Regression?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7ab5d7-829f-455c-a1e6-a826b4e2b263",
   "metadata": {},
   "source": [
    "### Ans -\n",
    "\n",
    "The assumptions of ridge regression are the same as that of linear regression: linearity, constant variance, and independence. However, as ridge regression does not provide confidence limits, the distribution of errors to be normal need not be assumed.\n",
    "\n",
    "1. Linearity - There is a linear relationship between the independent variable(s) and the dependent variable. \n",
    "2. Constant Variance - All the variables are normally distributed hence follows Gaussian distribution.\n",
    "3. Independence - The regression is the independence of observations. Independence means that there is no relation between the different examples. This is not something that can be deduced by looking at the data, but by plotting it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f04c3f3-e1dc-40c5-b648-ad3ece782eed",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-danger\"> Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f53976-acbe-4df9-88e3-376093a5804d",
   "metadata": {},
   "source": [
    "### Ans -\n",
    "\n",
    "A tuning parameter (λ), sometimes called a penalty parameter, controls the strength of the penalty term in ridge regression and lasso regression. It is basically the amount of shrinkage, where data values are shrunk towards a central point, like the mean. Shrinkage results in simple, sparse models which are easier to analyze than high-dimensional data models with large numbers of parameters.\n",
    "\n",
    "- When λ = 0, no parameters are eliminated. The estimate is equal to the one found with linear regression.\n",
    "- As λ increases, more and more coefficients are set to zero and eliminated.\n",
    "- When λ = ∞, all coefficients are eliminated.\n",
    "\n",
    "There is a trade-off between bias and variance in resulting estimators. As λ increases, bias increases and as λ decreases, variance increases. For example, setting your tuning parameter to a low value results in a more manageable number of model parameters and lower bias, but at the expense of a much larger variance.\n",
    "\n",
    "Following are the different criteria for selecting a Tuning Parameter\n",
    "1. Choose a regularization method. For example: ...\n",
    "2. Use a sequence of tuning parameters to create a series of different models.\n",
    "3. Study the different models and select one that best fits your needs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2694077-1c8f-4d66-978a-7692fab411f3",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-success\"> Q4. Can Ridge Regression be used for feature selection? If yes, how?</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64d6428-d5a6-4658-bd67-9b79d0ed05ba",
   "metadata": {},
   "source": [
    "### Ans- \n",
    "   One of the most important things about ridge regression is that without wasting any information about predictions it tries to determine variables that have exactly zero effects. Ridge regression is popular because it uses regularization for making predictions and regularization is intended to resolve the problem of overfitting.\n",
    "   We mainly find that overfitting is where the size of data is very large and ridge regression works by penalizing the coefficient of features and it also minimizes the errors in prediction. \n",
    "   \n",
    "   Following are the steps for Implementing Ridge regression for feature selection:\n",
    "1. Importing necessary libraries.\n",
    "2. Make two datasets as X(Dataset with Independent features) and y(Dependent feature).\n",
    "3. Plot the data e.g. scatterplot to see the distribution of the data.\n",
    "4. Split the data into train and test with sklean train_test_split.\n",
    "5. Scale the data with sklearn standardscaler.\n",
    "6. Create an instance of regression and fit it with trained data with features. \n",
    "4. In the above, we can see that the feature from the data is one of the most important features and other features are not that much important.coefficients w.r.t it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd659f5f-cb78-4442-bab8-4ee96ac3a45c",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-warning\">Q5. How does the Ridge Regression model perform in the presence of multicollinearity?</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e469cba1-c018-4710-b0ba-e65332db81f2",
   "metadata": {},
   "source": [
    "### Ans-\n",
    "With respect to ridge regression if two columns are highly correlated, it may make sense to keep one and remove the other. However, in some cases, both columns may be removed to avoid multicollinearity, which can cause issues in certain statistical models.\n",
    "\n",
    "Multicollinearity occurs when two or more predictor variables in a model are highly correlated with each other. This can cause problems when estimating the coefficients of the model, as it becomes difficult to determine the independent effect of each variable on the response variable. In other words, the model becomes unstable and the coefficients become imprecise.\n",
    "\n",
    "In the case, if two columns are highly correlated, it may make sense to remove both columns to avoid multicollinearity. By removing both columns, we can avoid the potential issues that may arise from including highly correlated variables in the model. Additionally, removing one variable and keeping the other may lead to loss of important information, especially if both variables are informative and contribute to the prediction of the response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174ce6ea-8d32-4cca-bbd0-1245edfdd304",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <div class=\"alert alert-info\">Q6. Can Ridge Regression handle both categorical and continuous independent variables?</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcfeb4d-9b87-4bc6-8693-af3195ac5273",
   "metadata": {},
   "source": [
    "### Ans-\n",
    "Yes, Ridge regression can handle both categorical and continuous independent variables. However, categorical variables need to be transformed into numerical variables before they can be used in a regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dfbb9e-0f2b-4440-9a4c-1bdc0d8f6089",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-danger\">Q7. How do you interpret the coefficients of Ridge Regression?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098ee92d-311e-4f1c-94a0-8a3c533c3a76",
   "metadata": {},
   "source": [
    "### Ans - \n",
    "To interpret the coefficients of Ridge regression, one can follow these steps:\n",
    "\n",
    "First, standardize the predictor variables to have zero mean and unit variance. This is important because Ridge regression is sensitive to the scale of the predictor variables.\n",
    "\n",
    "After fitting the Ridge regression model, examine the signs and magnitudes of the coefficients to determine which predictor variables are positively or negatively associated with the response variable. A positive coefficient indicates that an increase in the predictor variable is associated with an increase in the response variable, while a negative coefficient indicates that an increase in the predictor variable is associated with a decrease in the response variable.\n",
    "\n",
    "Compare the magnitudes of the coefficients to see which predictor variables have the greatest impact on the response variable. Keep in mind that the coefficients in Ridge regression have been shrunk towards zero, so smaller coefficients may still be important for the model.\n",
    "\n",
    "Additionally, one can use partial dependence plots to visualize the relationship between each predictor variable and the response variable, while holding all other predictor variables constant. This can help provide a more intuitive understanding of the relationship between the predictor variables and the response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6c8dfe-b6f9-4a74-afcb-329aacf6cb5d",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-success\">Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183256d6-fdc2-4224-b566-457281fdd82c",
   "metadata": {},
   "source": [
    "### Ans-\n",
    "Yes, Ridge regression can be used for time-series data analysis.\n",
    "\n",
    "One approach to using Ridge regression for time-series data analysis is to first transform the time-series data into a supervised learning problem by creating lagged variables. This involves creating new variables that represent the value of each variable at previous time points. For example, if we have a time series of monthly sales data, we could create lagged variables for the previous month's sales, the sales from two months ago, and so on.\n",
    "\n",
    "Once the data has been transformed into a supervised learning problem, Ridge regression can be used to model the relationship between the lagged variables and the response variable (i.e., the current month's sales). The L2 penalty term added to the objective function in Ridge regression can help to reduce the variance of the coefficients and improve the stability of the model, especially in cases where the time series data has a high degree of autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e17d50-dd69-42f8-8d0e-190ddaa11fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
