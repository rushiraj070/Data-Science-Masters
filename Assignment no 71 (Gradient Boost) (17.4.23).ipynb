{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "560cbbfd-08e5-4744-a3af-f28566114837",
   "metadata": {},
   "source": [
    "# Assignment no 71 (Gradient Boost) (17.4.23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704b4636-6303-40e4-91a4-7baa8de64460",
   "metadata": {},
   "source": [
    "### Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "**Ans-** \n",
    "- Gradient Boosting Regression is an analytical technique that is designed to explore the relationship between two or more variables (X, and Y). \n",
    "- Its analytical output identifies important factors (Xi) impacting the dependent variable (y) and the nature of the relationship between each of these factors and the dependent variable. It is a machine learning technique used in regression tasks, among others. \n",
    "- It gives a prediction model in the form of an ensemble of weak prediction models, which are typically decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c4d048-dca7-4f7d-afe2-7f1f28d24c08",
   "metadata": {},
   "source": [
    "### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "041b6a77-a5e4-4bd4-b2d5-297a661ed682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "raw",
   "id": "30b843f9-1dd7-4e12-8ede-77814efe18dd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Define the loss function\n",
    "def loss(y_true, y_pred):\n",
    "    return y_true - y_pred\n",
    "\n",
    "# Define the base learner\n",
    "def base_learner(X, y, w):\n",
    "    return np.sum(w * X * y) / np.sum(w * X * X)\n",
    "\n",
    "# Define the gradient boosting algorithm\n",
    "def gradient_boosting(X, y, M=10, learning_rate=0.1):\n",
    "    N = len(y)\n",
    "    F = np.zeros(N)\n",
    "    w = np.ones(N) / N\n",
    "    models = []\n",
    "    for m in range(M):\n",
    "        # Compute the pseudo-residuals\n",
    "        residuals = loss(y, F)\n",
    "        # Fit a base learner to the pseudo-residuals\n",
    "        model = base_learner(X, residuals, w)\n",
    "        # Update the model\n",
    "        F += learning_rate * model * X\n",
    "        # Store the model\n",
    "        models.append(model)\n",
    "    return models\n",
    "\n",
    "# Define the prediction function\n",
    "def predict(models, X, learning_rate=0.1):\n",
    "    y_pred = np.zeros(len(X))\n",
    "    for model in models:\n",
    "        y_pred += learning_rate * model * X\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69de6542-3923-4cc3-9e34-d20356d7df7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some example data\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_features=4, n_informative=2, random_state=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b715473d-cd22-4a41-814b-b2f64be62767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fe22940-b3c1-48aa-8e4c-6241681bb4ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the gradient boosting model\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbr = GradientBoostingRegressor() # creating an object of Gradient Boost Regressor\n",
    "\n",
    "gbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "440f4edc-6af8-442a-8cae-87c05558b245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-11.49735754,  -4.03523658,  38.13667388, -53.73729762,\n",
       "       -42.10841813, -10.70259687,  34.83090542, -43.75183739,\n",
       "       -62.53762241,  47.20340744, -10.48511194, -31.101997  ,\n",
       "        -3.09548915, -18.90302248,  10.89834179,  13.8306622 ,\n",
       "       -22.78335407,   9.18346273, -28.16330545, -19.91097178,\n",
       "       -29.35283188,  -4.33553175,  39.49646116,  36.87669352,\n",
       "         2.97184675, -27.37768328,  46.95889601, -56.20913972,\n",
       "        79.56187483,  36.48045401])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "\n",
    "y_pred = gbr.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7665f4f-9e32-460c-9efd-5532bcfc8679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error for Gradient Boost Regressor is 85.6359.\n",
      "R-squared error for Gradient Boost Regressor is 0.9339.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's performance\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean squared error for Gradient Boost Regressor is {mse:.4f}.\")\n",
    "print(f\"R-squared error for Gradient Boost Regressor is {r2:.4f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff94b35-ccd0-40ad-81f7-f14848a4c017",
   "metadata": {},
   "source": [
    "### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1211da7-74be-4322-aa19-bb9768d2240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32a2bbc9-b458-47fe-a332-f6dd34f69827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the hyperparameter grid\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [1, 2, 3, 4]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70218814-7bea-4f77-a9e5-89991c11c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid search object\n",
    "\n",
    "grid_search = GridSearchCV(gbr, param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca472e3b-c954-48f8-9d6b-397e143ec29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.01, 0.05, 0.1, 0.2],\n",
       "                         &#x27;max_depth&#x27;: [1, 2, 3, 4],\n",
       "                         &#x27;n_estimators&#x27;: [10, 50, 100, 200]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.01, 0.05, 0.1, 0.2],\n",
       "                         &#x27;max_depth&#x27;: [1, 2, 3, 4],\n",
       "                         &#x27;n_estimators&#x27;: [10, 50, 100, 200]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n",
       "             param_grid={'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
       "                         'max_depth': [1, 2, 3, 4],\n",
       "                         'n_estimators': [10, 50, 100, 200]})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the grid search object to the data\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "860325fd-97c1-4af8-bef6-f13e45ac0cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyperparameters for our gradient boost regression model are {'learning_rate': 0.2, 'max_depth': 1, 'n_estimators': 200}.\n"
     ]
    }
   ],
   "source": [
    "# Print the best hyperparameters\n",
    "\n",
    "print(f\"The best hyperparameters for our gradient boost regression model are {grid_search.best_params_}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af458e86-3cfb-46ed-a61e-46f4e7c5679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set using the best model\n",
    "\n",
    "y_pred = grid_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed47191e-5e0f-4d60-b579-c1be63e9095b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error for Gradient Boost Regressor is 43.9197\n",
      "\n",
      "R-squared error for Gradient Boost Regressor is 0.9661\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's performance\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean squared error for Gradient Boost Regressor is {mse:.4f}\")\n",
    "print()\n",
    "print(f\"R-squared error for Gradient Boost Regressor is {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187798fa-e2b4-425d-942a-b18abafe65be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e1437b1-e644-4139-b4d5-55dca8e7cc7b",
   "metadata": {},
   "source": [
    "### Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "**Ans-** \n",
    "1. A weak learner is a simple model that performs only slightly better than random chance. \n",
    "2. In the context of gradient boosting, *weak learners are combined iteratively to form a strong learner*. \n",
    "3. The idea is to start with a single weak learner, typically a decision tree, and then train additional weak learners that focus on the areas where the previous weak learners performed poorly. 4. This process continues until a predetermined stopping condition is met, such as a set number of weak learners have been created or the model's performance has plateaued.\n",
    "\n",
    "**The goal of boosting is:**\n",
    "1. To make small adjustments to the prediction function gradually, evolving its shape in a slow and controlled manner to combat overfitting. \n",
    "2. Building the complex predictive function is the job of the boosting algorithm, not the weak learner being boosted. \n",
    "\n",
    "This approach allows for the creation of a strong learner from many simple models, rather than relying on a single complex model that may be prone to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8249d61a-8f32-4db9-bd46-8d8142a55a19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0a4cd56-1ac8-458f-9771-13b87b61eb56",
   "metadata": {},
   "source": [
    "### Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "**Ans-** \n",
    "The intuition behind the Gradient Boosting algorithm is to iteratively improve the predictions of a model by training additional models that focus on the areas where the previous models performed poorly. This is done by combining multiple weak learners, which are simple models that perform only slightly better than random chance, to form a strong learner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1db4d9-51c9-465f-a0e1-515913c99e62",
   "metadata": {},
   "source": [
    "### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "**Ans-** The Gradient Boosting algorithm builds an ensemble of weak learners by iteratively training additional models that focus on the areas where the previous models performed poorly. This is done by combining multiple weak learners, which are simple models that perform only slightly better than random chance, to form a strong learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f15f18-0454-4351-b2c8-04b991b0fb60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8b1a520-f229-40a1-ac2f-72125874b7b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "\n",
    "**Ans-**\n",
    "The mathematical intuition behind the Gradient Boosting algorithm involves several steps. Here is a brief overview of the process:\n",
    "\n",
    "1. Define an initial model: An initial model m0 is defined to predict the target variable y. This model will be associated with a residual (y – R0).\n",
    "\n",
    "2. Fit a new model to the residuals: A new model m1 is fit to the residuals from the previous step.\n",
    "\n",
    "3. Combine the models: Now, R0 and m1 are combined to give R1, the boosted version of R0. The mean squared error from m1 will be lower than that from m0.\n",
    "\n",
    "The process is repeated iteratively, with each new model being fit to the residuals of the previous model and then combined with the previous models to form a stronger learner. The goal is to gradually improve the predictions of the model by making small adjustments to the prediction function in a slow and controlled manner, in order to combat overfitting2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fa58f3-43c2-4b9b-9485-2b49d83e4aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
