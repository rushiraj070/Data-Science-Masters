{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27a4e03d-2584-4eef-aa08-8f9b266fed9c",
   "metadata": {},
   "source": [
    "# <div class=\"alert alert-warning\">Assignment no 56 (Logistic Regression) (2.4.23) <div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c690e2-06f1-4368-b098-bd879e97984b",
   "metadata": {},
   "source": [
    "## Cross Validation, Confusion Matrix, Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7035bf6-d28a-4a66-8809-90e558d4f5a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <div class=\"alert alert-success\">Q1. What is the purpose of grid search cv in machine learning, and how does it work?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0350610f-0ede-4e86-a40b-8fc29200270a",
   "metadata": {},
   "source": [
    "   \n",
    "**Ans-** GridSearchCV is a combination of GridSearch with Cross Validation.\n",
    "    \n",
    "Purpose in Machine Learning -\n",
    " - The main purpose GridSearchCV is to train the model by finding out the right combination of the various and best parameters for maximum accuracy with GridSearch such that it avoids overfitting in model and then Cross Validation is used for Hyperparameter Tuning within the train model.\n",
    " \n",
    "<div class=\"alert alert-warning\">Generally speaking Cross-Validation is VALIDATION within VALIDATION or ACCURACY within ACCURACY.</div>\n",
    " \n",
    "Working - \n",
    " 1. First use GridSearch to find out right combination between the selection regression type for maximum accuracy.\n",
    "     e.g. If within Logistic Regression we have selected certain parameters like penalty('l1', 'l2','elasticnet'), C[10,20,30] and multi_class'['auto', 'ovr', 'multinomial'] so GridSearch will try each combination and give best parameter combination.\n",
    "     \n",
    " 2. Then with above parameters we will train our model on train data.\n",
    " \n",
    " 3. Then Cross Validation will again divide train data into training and validation e.g. like KFold and again find best accuracy with no of folds e.g. For cv=5 we will get 5 different accuracies and finally we will average it     as our final accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d54b57-0899-4f08-bce1-b2a73ec27839",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-danger\">Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose one over the other? </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f1a493-9543-4c7c-9761-df30031744f1",
   "metadata": {},
   "source": [
    "\n",
    "**Ans-**\n",
    "\n",
    "- **GridSearchCV -**\n",
    "      It involves defining a range of values for each hyperparameter and evaluating all possible combinations of         these hyperparameters using cross-validation. This can be computationally expensive and time-consuming,             particularly when there are a large number of hyperparameters or when the range of values for each                 hyperparameter is large.\n",
    "\n",
    "- **RandomizedSearchCV -**\n",
    "      It randomly samples hyperparameters from defined probability distributions, and then evaluates these randomly       sampled combinations using cross-validation. Randomized search can be more computationally efficient than           grid search because it samples only a small subset of all possible hyperparameter combinations.\n",
    "\n",
    "- **Logic for Selection -** \n",
    "When choosing between grid search and randomized search, there are a few factors to consider. If the number of hyperparameters is small and the range of values for each hyperparameter is also small, grid search might be a good option. However, if the number of hyperparameters is large or the range of values for each hyperparameter is large, randomized search might be a better option because it can explore a larger range of hyperparameters in a shorter amount of time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c83946-84ee-40c8-9f15-29ef169ee2da",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-info\">Q3. What is data leakage, and why is it a problem in machine learning? Provide an example.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecfc9ba-90a6-45c0-b989-4e1bb63e4c2c",
   "metadata": {},
   "source": [
    "   \n",
    "**Ans -** \n",
    "- Data Leakage is the scenario where the Machine Learning Model is already aware of some part of test data after training which causes the problem of overfitting.\n",
    "- In Machine learning, Data Leakage refers to a mistake that is made by the creator of a machine learning model in which they accidentally share the information between the test and training data sets. \n",
    "- Typically, when splitting a data set into testing and training sets, the goal is to ensure that no data is shared between these two sets. Ideally, there is no intersection between these two sets. This is because the purpose of the testing set is to simulate the real-world data which is unseen to that model. \n",
    "- However, when evaluating a model, we do have full access to both our train and test sets, so it is our duty to ensure that there is no overlapping between the training data and the testing data (i.e, no intersection).\n",
    "- In simple terms, Data Leakage occurs when the data used in the training process contains information about what the model is trying to predict. It appears like “cheating” but since we are not aware of it so, it is better to call it “leakage” instead of cheating. Therefore, Data leakage is a serious and widespread problem in data mining and machine learning which needs to be handled well to obtain a robust and generalized predictive model.\n",
    "\n",
    "**Example -**\n",
    "The most obvious and easy-to-understand cause of data leakage is to include the target variable as a feature. What happens is that after including the target variable as a feature, our purpose of prediction got destroyed. This is likely to be done by mistake but while modelling any ML model, you have to make sure that the target variable is differentiated from the set of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0beed34-0578-49b8-a095-9040a997e983",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-warning\">Q4. How can you prevent data leakage when building a machine learning model?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6049f4-3fe5-4472-bea9-58b25d030102",
   "metadata": {},
   "source": [
    "\n",
    "**Ans -**\n",
    "Here are some techniques that can help prevent data leakage:\n",
    "\n",
    "1. Split data into training, validation, and test sets: It is essential to split the data into separate sets for training, validation, and testing. This ensures that the model is not exposed to the test data during training, and that the model's performance on the test data is a fair evaluation of its ability to generalize to new data.\n",
    "\n",
    "2. Use cross-validation: Cross-validation is a technique that involves splitting the data into multiple subsets and training the model on different combinations of these subsets. This can help to prevent data leakage by ensuring that the model is evaluated on data that it has not seen during training.\n",
    "\n",
    "3. Avoid using future data for training: When working with time series data, it is important to ensure that the model is not trained on future data that would not be available during model deployment. One way to prevent this is to use a rolling window approach, where the model is trained on data up to a certain point in time and tested on data from a later time period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6872f4b-6954-4398-99e2-e149b3b79531",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-success\">Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d363b65-bb7b-43d7-898a-3bfb5aa2764d",
   "metadata": {},
   "source": [
    "\n",
    "**Ans -**\n",
    "\n",
    "A confusion matrix is a table that summarizes the performance of a classification model by comparing the predicted class labels to the true class labels. It is commonly used in machine learning to evaluate the performance of binary and multiclass classification models.\n",
    "\n",
    "A confusion matrix is typically organized as follows:\n",
    "\n",
    "![download.png](attachment:d54fb49e-c9ad-4dbf-b479-97828ca4dea8.png)\n",
    "\n",
    "The confusion matrix provides valuable information about the performance of a classification model, including:\n",
    "\n",
    "- **Accuracy:** The overall proportion of correct predictions made by the model, which is calculated as (TP+TN)/(TP+TN+FP+FN).\n",
    "\n",
    "- **Precision:** The proportion of true positive predictions out of all positive predictions, which is calculated as TP/(TP+FP).\n",
    "\n",
    "- **Recall (also known as sensitivity):** The proportion of true positive predictions out of all actual positive instances, which is calculated as TP/(TP+FN).\n",
    "\n",
    "- **F1-score:** A weighted average of precision and recall that takes into account both measures. It is calculated as 2 * (precision * recall) / (precision + recall)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8675dd67-b470-4d8b-b066-95b20615b23b",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-danger\">Q6. Explain the difference between precision and recall in the context of a confusion matrix.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd177eb-2089-4ba9-a80a-02fa7334d8b6",
   "metadata": {},
   "source": [
    "\n",
    "**Ans -**\n",
    "\n",
    "- Precision measures the proportion of true positives (TP) among all instances predicted as positive (both true positives and false positives). It can be interpreted as the ability of the model to correctly identify positive instances, without falsely labeling too many negative instances as positive. Precision is calculated as TP / (TP + FP), where FP is the number of false positives.\n",
    "\n",
    "- Recall, also known as sensitivity, measures the proportion of true positives (TP) among all actual positive instances (both true positives and false negatives). It can be interpreted as the ability of the model to correctly identify all positive instances, without missing too many of them. Recall is calculated as TP / (TP + FN), where FN is the number of false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e50e40-8bda-4bef-bd4c-6a49d2b4ff91",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-info\">Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e5f9c1-c9a5-4afb-91e2-72cc140bf1b2",
   "metadata": {},
   "source": [
    "\n",
    "**Ans -**\n",
    "\n",
    "A confusion matrix summarizes the performance of a classification model by comparing the predicted class labels to the true class labels. It provides valuable information about the types of errors the model is making, which can help identify areas for improvement. Here is how to interpret a confusion matrix to determine which types of errors your model is making:\n",
    "\n",
    "**1. Identify the classes:** A confusion matrix is organized into rows and columns that represent the predicted and actual class labels, respectively. Identify which classes your model is predicting and which classes it is supposed to predict.\n",
    "\n",
    "**2. Calculate the metrics:** Use the counts in each cell of the matrix to calculate various metrics, such as accuracy, precision, recall, and F1-score. These metrics provide an overall view of how well the model is performing.\n",
    "\n",
    "**3. Examine the errors:** Look at the cells that represent misclassifications to identify which types of errors the model is making.\n",
    "\n",
    "**4. Analyze the errors:** Examine the errors to determine what might be causing them. For example, false positives might be caused by noisy data or an overly complex model, while false negatives might be caused by a lack of features or an overly simplistic model.\n",
    "\n",
    "**5. Adjust the model:** Use the insights gained from analyzing the errors to adjust the model and improve its performance. This might involve changing the algorithm, adjusting the hyperparameters, or adding more features to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21a8562-46ba-4812-8926-92c9ab65380a",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-warning\">Q8. What are some common metrics that can be derived from a confusion matrix, and how are they calculated? </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f72e27e-5454-4a6c-bece-628dc09fa63c",
   "metadata": {},
   "source": [
    "\n",
    "**Ans -**\n",
    "Here are some of the most common ones:\n",
    "\n",
    "- **Accuracy:** This metric measures the proportion of correct predictions over the total number of predictions, regardless of class. It is calculated as (TP + TN) / (TP + TN + FP + FN).\n",
    "\n",
    "- **Precision:** This metric measures the proportion of true positives among all positive predictions. It is calculated as TP / (TP + FP).\n",
    "\n",
    "- **Recall (also known as sensitivity):** This metric measures the proportion of true positives among all actual positive instances. It is calculated as TP / (TP + FN).\n",
    "\n",
    "- **F1-score:** This metric is the harmonic mean of precision and recall and provides a balanced evaluation of a classifier's performance. It is calculated as 2 * (precision * recall) / (precision + recall).\n",
    "\n",
    "- **Specificity:** This metric measures the proportion of true negatives among all actual negative instances. It is calculated as TN / (TN + FP).\n",
    "\n",
    "- **False positive rate:** This metric measures the proportion of false positives among all actual negative instances. It is calculated as FP / (TN + FP).\n",
    "\n",
    "- **False negative rate:** This metric measures the proportion of false negatives among all actual positive instances. It is calculated as FN / (TP + FN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bab37d4-6a3d-48a8-ad60-d65295627c4c",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-success\">Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix? </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f53b8d6-5a51-49e9-8011-839f941e5902",
   "metadata": {},
   "source": [
    "\n",
    "**Ans -**\n",
    "\n",
    "The accuracy of a classification model is closely related to the values in its confusion matrix, as it is one of the most common metrics derived from the confusion matrix. Accuracy measures the proportion of correct predictions made by the model across all classes and is calculated as (TP + TN) / (TP + TN + FP + FN), where TP, TN, FP, and FN are the counts of true positives, true negatives, false positives, and false negatives, respectively.\n",
    "\n",
    "A model with a high accuracy score generally has a higher count of true positives and true negatives and a lower count of false positives and false negatives in its confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea955b97-d191-4e0c-a08a-75a3b48257d1",
   "metadata": {},
   "source": [
    "### <div class=\"alert alert-danger\">Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning model? </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9817f2b2-5701-425d-90f9-d2ec14c8b650",
   "metadata": {},
   "source": [
    "\n",
    "**Ans -**\n",
    "\n",
    "Here are some ways to use a confusion matrix to identify such biases or limitations:\n",
    "\n",
    "**1. Class imbalance:** A confusion matrix can help identify class imbalance, where the number of instances in one class is significantly higher or lower than the other classes. This can be seen by comparing the counts of true positives, true negatives, false positives, and false negatives for each class. A model that performs well on the majority class but poorly on the minority class might indicate class imbalance.\n",
    "\n",
    "**2. Misclassification patterns:** Examining the false positive and false negative rates for each class in the confusion matrix can help identify misclassification patterns. For example, if the model consistently misclassifies instances from one class as another class, this might indicate a problem with feature selection or model architecture.\n",
    "\n",
    "**3. Performance across classes:** Comparing the precision and recall scores across classes can help identify performance differences. For example, a model with high precision but low recall might indicate that the model is biased towards predicting negative instances, which can be problematic if the positive class is of particular interest.\n",
    "\n",
    "**4. Limitations of the model:** A confusion matrix can help identify limitations of the model in terms of the types of errors it makes. For example, if the model has a high false positive rate, this might indicate that the model is too sensitive to certain features and is prone to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbe416c-e321-4201-9d22-04e8e4317f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
