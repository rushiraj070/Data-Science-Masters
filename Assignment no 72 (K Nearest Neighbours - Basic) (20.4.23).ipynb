{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57f7e0e3-b2d7-4331-826c-9820279e97e9",
   "metadata": {},
   "source": [
    "# Assignment no 72 (K Nearest Neighbours - Basic) (20.4.23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050119c0-f5f5-4767-89cb-66c6cccc8846",
   "metadata": {},
   "source": [
    "### Q1. What is the KNN algorithm?\n",
    "\n",
    "**Ans -** \n",
    "- The K-Nearest Neighbor (KNN) algorithm is a simple, yet powerful, supervised learning technique used for classification and regression problems. \n",
    "- It is a non-parametric algorithm, meaning it does not make any assumptions about the underlying data distribution. \n",
    "- The KNN algorithm works by assuming similarity between new data points and available data points, and classifies the new data point into the category that is most similar to the available categories. \n",
    "- It is also called a lazy learner algorithm because it does not learn from the training set immediately, instead it stores the dataset and performs an action on the dataset at the time of classification and can be used for both regression and classification problems, but it is mostly used for classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0b7091-df58-4bd0-b28c-c5bb0fd05bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "153f0711-800b-4c92-a534-a0f495c2529c",
   "metadata": {},
   "source": [
    "### Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "**Ans -** \n",
    "\n",
    "Choosing the value of K in the KNN algorithm is an important step as it can significantly impact the performance of the model. There are several methods to determine the optimal value of K for a given dataset. Here are some common techniques used to choose the value of K:\n",
    "\n",
    "1. **Cross-validation**: One way to determine the optimal value of K is to use cross-validation. This involves splitting the dataset into training and validation sets, and then testing different values of K on the validation set to see which value gives the best performance.\n",
    "\n",
    "2. **The square root rule**: Another simple approach to select K is to set K = sqrt(n), where n = total number of data points in the dataset.\n",
    "\n",
    "3. **The elbow method**: This method involves plotting the error rate for different values of K and choosing the value of K where the error rate starts to decrease more slowly (the \"elbow\" point).\n",
    "\n",
    "It is also important to note that **choosing a very low value of K can lead to inaccurate predictions due to noise in the data, while choosing a very high value can make the model computationally expensive.**\n",
    "\n",
    "It is generally recommended to **choose an odd value for K if there are only two classes in the dataset, to avoid ties in classification.**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "13e4c565-4c9f-4a4c-bb2b-7ed240de06d4",
   "metadata": {},
   "source": [
    "Source: Conversation with Bing, 13/8/2023\n",
    "(1) k nn - How to determine the number of K in KNN - Data Science Stack .... https://datascience.stackexchange.com/questions/102983/how-to-determine-the-number-of-k-in-knn.\n",
    "(2) K-Nearest Neighbor(KNN) Algorithm - GeeksforGeeks. https://www.geeksforgeeks.org/k-nearest-neighbours/.\n",
    "(3) How to choose the value of K in knn algorithm - techniques - Data .... https://discuss.analyticsvidhya.com/t/how-to-choose-the-value-of-k-in-knn-algorithm/2606.\n",
    "(4) KNN Algorithm – K-Nearest Neighbors Classifiers and Model Example. https://www.freecodecamp.org/news/k-nearest-neighbors-algorithm-classifiers-and-model-example/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d09e47-3e12-44e1-a148-050002abef33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f48d11b-a256-4ffd-8258-7f91694e5c85",
   "metadata": {},
   "source": [
    "### Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "**Ans -** \n",
    "The KNN algorithm can be used for both classification and regression problems. The main difference between the KNN classifier and the KNN regressor is the way they make predictions based on the nearest neighbors.\n",
    "\n",
    "**1. KNN Classifier:** The KNN classifier predicts the class of a new data point by taking a majority vote among its K nearest neighbors. In other words, it assigns the new data point to the class that is most common among its K nearest neighbors.\n",
    "\n",
    "**2. KNN Regressor:** The KNN regressor, on the other hand, predicts a continuous value for a new data point by taking the average of the values of its K nearest neighbors.\n",
    "\n",
    "In summary, the KNN classifier is used for classification problems where the output variable is categorical, while the KNN regressor is used for regression problems where the output variable is continuous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b3dd66-17f2-4964-96c0-7d5a4e97d8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd566123-8388-477f-b81f-2f2b646b0049",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Q4. How do you measure the performance of KNN?\n",
    "\n",
    "**Ans -** \n",
    "The performance of the KNN algorithm can be measured using various evaluation metrics, depending on the type of problem (classification or regression) and the specific requirements of the task.\n",
    "\n",
    "For classification problems, some common evaluation metrics used to measure the performance of the KNN classifier include **accuracy, precision, recall, F1-score, and confusion matrix**. These metrics can be calculated by comparing the predicted class labels with the true class labels of the test data.\n",
    "\n",
    "For regression problems, some common evaluation metrics used to measure the performance of the KNN regressor include **mean squared error (MSE), root mean squared error (RMSE), mean absolute error (MAE), and R-squared**. These metrics can be calculated by comparing the predicted continuous values with the true values of the test data.\n",
    "\n",
    "It is important to note that when training a KNN classifier or regressor, it is **essential to normalize the features as KNN measures the distance between points**. The test data is used to evaluate the performance of the model by making predictions and comparing these predictions to the actual target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbca482-af7e-429d-ba86-f223212af6af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "650d0b9d-4e8f-456b-8d47-39621eb92840",
   "metadata": {},
   "source": [
    "### Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "**Ans -** The curse of dimensionality is a phenomenon that occurs when the number of features (dimensions) in a dataset increases, causing the feature space to become increasingly sparse. This can have a significant impact on the performance of the KNN algorithm, as it relies on measuring the distance between data points to make predictions.\n",
    "\n",
    "In high-dimensional spaces, the distance between data points can become less meaningful, as all points tend to be almost equidistant from each other. This can make it difficult for the KNN algorithm to accurately identify the nearest neighbors of a given data point, and can result in inaccurate predictions.\n",
    "\n",
    "To overcome the curse of dimensionality in KNN, it is often necessary to use techniques such as dimensionality reduction to reduce the number of features in the dataset. This can help to improve the performance of the KNN algorithm by making the feature space less sparse and allowing the algorithm to more accurately identify the nearest neighbors of a given data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8c6d2c-c446-45b5-86c2-242819ac5e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8d23820-2850-45ad-a608-be4d7e51528b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Q6. How do you handle missing values in KNN?\n",
    "\n",
    "**Ans -** Missing values can cause problems for many machine learning algorithms, including the KNN algorithm. As such, it is important to identify and handle missing values in the dataset prior to modeling your prediction task. This is called missing data imputation, or imputing for short.\n",
    "\n",
    "One popular approach to missing data imputation is to use a model to predict the missing values. This requires a model to be created for each input variable that has missing values. Although any one among a range of different models can be used to predict the missing values, the k-nearest neighbor (KNN) algorithm has proven to be generally effective, often referred to as “nearest neighbor imputation”.\n",
    "\n",
    "The KNN imputation method works by finding the K nearest neighbors of the data point with missing values, based on the other variables in the dataset. The missing value is then imputed by taking the average (for continuous variables) or the mode (for categorical variables) of the K nearest neighbors.\n",
    "\n",
    "In Python, you can use the KNN Imputer class from the scikit-learn library to perform KNN imputation on your dataset. This class provides an easy-to-use interface for filling in missing values using the KNN algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c333e2-fdcb-40ed-970d-cba29cffbff8",
   "metadata": {},
   "source": [
    "### Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "\n",
    "**Ans -** The KNN classifier and regressor are two different algorithms that are used for different types of problems. The KNN classifier is used for classification problems, where the goal is to predict the class or category that a data point belongs to. On the other hand, the KNN regressor is used for regression problems, where the goal is to predict a continuous value.\n",
    "\n",
    "The performance of these two algorithms depends on the type of problem they are being used for. For classification problems, the KNN classifier is generally a better choice, as it is designed to predict classes. For regression problems, the KNN regressor is generally a better choice, as it is designed to predict continuous values.\n",
    "\n",
    "In summary, the choice between the KNN classifier and regressor depends on the type of problem being solved. For classification problems, the KNN classifier is generally a better choice, while for regression problems, the KNN regressor is generally a better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474fd3c3-85c3-4003-b709-fe2c2f88fc57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d172016-cff0-4cd7-9f77-1517df7c9ac8",
   "metadata": {},
   "source": [
    "### Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "\n",
    "**Ans -** \n",
    "\n",
    "The k-Nearest Neighbor (kNN) algorithm is a simple but effective machine learning algorithm that can be used for both classification and regression tasks. \n",
    "\n",
    "Some of the strengths of the kNN algorithm include its \n",
    "1. flexibility, \n",
    "2. ease of implementation, and \n",
    "3. robustness to noisy data.\n",
    "\n",
    "However, the kNN algorithm also has some weaknesses. \n",
    "1. One weakness is that it can be computationally expensive, especially when dealing with large datasets. \n",
    "2. Another weakness is that it can be sensitive to the choice of the number of nearest neighbors (k) and the distance metric used.\n",
    "\n",
    "To address these weaknesses, several modified versions of the kNN algorithm have been developed. These variants aim to remove the weaknesses of kNN and provide a more efficient method. For example, one approach is to use dimensionality reduction techniques to reduce the computational cost of the algorithm. Another approach is to use cross-validation or other methods to select the optimal value of k and the distance metric.\n",
    "\n",
    "In summary, while the kNN algorithm has several strengths, it also has some weaknesses that can be addressed through various techniques and modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e5f3c5-eadb-4b69-91bd-25010e16e3e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "397b8605-a69b-4018-8755-2a10cf801dea",
   "metadata": {},
   "source": [
    "### Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "**Ans -** In the context of the k-Nearest Neighbor (kNN) algorithm, *Euclidean distance and Manhattan distance are two different distance metrics that can be used to calculate the distance between data points.* The choice of distance metric can affect the performance of the kNN algorithm, so it is important to understand the differences between these two metrics.\n",
    "\n",
    "**1. Euclidean distance:** is the straight-line distance between two points in Euclidean space. It is calculated using the Pythagorean theorem, which states that the square of the distance between two points is equal to the sum of the squares of the differences between their coordinates.\n",
    "\n",
    "**2. Manhattan distance:** is also known as taxicab distance or city block distance. It is calculated as the sum of the absolute differences between the coordinates of two points. This distance metric is preferred over Euclidean distance when dealing with high dimensionality.\n",
    "\n",
    "In summary, Euclidean and Manhattan distances are two different distance metrics that can be used in kNN. Euclidean distance measures the straight-line distance between two points, while Manhattan distance measures the sum of the absolute differences between their coordinates. The choice of distance metric can affect the performance of kNN, so it is important to choose the appropriate metric for a given problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5015ae71-efd9-459b-93f2-826e1a439a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ca2e0fd-3226-4580-b942-d4b4335c22b4",
   "metadata": {},
   "source": [
    "### Q10. What is the role of feature scaling in KNN?\n",
    "\n",
    "**Ans -** \n",
    "\n",
    "Feature scaling is an important preprocessing step for many machine learning algorithms, including the k-Nearest Neighbor (kNN) algorithm. It involves rescaling each feature such that it has a standard deviation of 1 and a mean of 0. This is crucial for the kNN algorithm, as it helps in preventing features with larger magnitudes from dominating the distance calculations.\n",
    "\n",
    "The kNN algorithm is a distance-based algorithm, which means that it calculates the distance between data points to determine their similarity. If the features have different scales, then the distance calculation can be dominated by the features with larger magnitudes. This can lead to suboptimal performance of the kNN algorithm.\n",
    "\n",
    "To prevent this from happening, it is important to scale the data before applying the kNN algorithm. This can be done using techniques such as normalization or standardization. By scaling the data, all features are brought to the same scale, which ensures that no single feature dominates the distance calculation. This can improve the performance of the kNN algorithm and lead to more accurate predictions.\n",
    "\n",
    "In summary, feature scaling is an important step when using the kNN algorithm, as it helps to prevent features with larger magnitudes from dominating the distance calculations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ed0de5-469c-4d99-8db7-e743692d2a14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
